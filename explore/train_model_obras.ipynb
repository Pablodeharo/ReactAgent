{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4f83622",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3a6be40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬øCUDA disponible?: True\n",
      "N√∫mero de GPUs: 1\n",
      "Nombre de la GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Configurar dispositivo como variable global\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"¬øCUDA disponible?:\", torch.cuda.is_available())\n",
    "print(\"N√∫mero de GPUs:\", torch.cuda.device_count())\n",
    "print(\"Nombre de la GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No hay GPU disponible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bae4cd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memoria GPU asignada: 585.35 MB\n",
      "Memoria GPU reservada: 630.00 MB\n",
      "Memoria libre   : 3510.21 MB\n"
     ]
    }
   ],
   "source": [
    "def check_gpu_memory():\n",
    "    allocated_memory = torch.cuda.memory_allocated()\n",
    "    cached_memory = torch.cuda.memory_reserved()\n",
    "    print(f\"Memoria GPU asignada: {allocated_memory / 1024 ** 2:.2f} MB\")\n",
    "    print(f\"Memoria GPU reservada: {cached_memory / 1024 ** 2:.2f} MB\")\n",
    "    print(f\"Memoria libre   : {torch.cuda.get_device_properties(0).total_memory / 1024 ** 2 - allocated_memory / 1024 ** 2:.2f} MB\")\n",
    "\n",
    "check_gpu_memory()  # Verifica antes de entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5446a227",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\Lucas\\Desktop\\Socrates\\llm_proyect\\data\\procesed\\obras_socraticos_procesados.csv\"\n",
    "df_obras = pd.read_csv(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4bbefef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     clitofon, acusado por s√≥crates de haber censur...\n",
      "1     la correspondencia que sigue comprende trece c...\n",
      "2     ¬øqu√© es lo justo? este es el comienzo , y el a...\n",
      "3     h√© aqu√≠ otra serie de preguntas y respuestas d...\n",
      "4     la vida no es m√°s que un viaje; y es natural, ...\n",
      "5     el , muy superior a los di√°logos inmediatament...\n",
      "6     este di√°logo no deja de tener analog√≠a con el ...\n",
      "7     quiere decir complemento o conclusi√≥n de las l...\n",
      "8     timeo, nacido entre los locrios epicefirianos ...\n",
      "9     plat√≥n, o el autor, cualquiera que √©l sea, de ...\n",
      "10    un pormenor interesante relativo a hiparco, hi...\n",
      "11    demodoco, padre del joven teages, suplica a s√≥...\n",
      "12    ateniense,si alguno usurpa, cerca de un gobier...\n",
      "13    no debe orarse ligeramente. dirigir a los dios...\n",
      "14    sin gracia en la forma, sin verdad y sin inter...\n",
      "15    ateniense,se trata ahora de formar las leyes c...\n",
      "16    ateniense,el orden de materias nos conduce a l...\n",
      "17    ateniense,el orden natural de nuestras leyes n...\n",
      "18    ateniense,despu√©s de lo que se acaba de decir ...\n",
      "19    ateniense,es tiempo, despu√©s de todo lo que ac...\n",
      "20    despu√©s del nacimiento de los hijos de ambos s...\n",
      "21    ateniense,vosotros, que hab√©is oido lo que he ...\n",
      "22    ateniense,dime, te lo suplico; ¬øqu√© idea debem...\n",
      "23    ateniense,basta ya sobre este asunto, y ahora ...\n",
      "24    ateniense,extranjeros, ¬øqui√©n pasa entre vosot...\n",
      "25    ateniense,es indispensable, a mi parecer, exam...\n",
      "26    el , comienza , puede decirse que el , contin√∫...\n",
      "27    el , no es, como los precedentes di√°logos, una...\n",
      "28    este di√°logo tiene poco de tal. s√≥crates, crit...\n",
      "29    definido el sofista, falta hacerlo mismo con e...\n",
      "30    el objeto de este di√°logo es el amor. he aqu√≠ ...\n",
      "31    igual al , por la fuerza y elevaci√≥n moral de ...\n",
      "32    los nombres tienen una propiedad; es natural o...\n",
      "33    plat√≥n, en este peque√±o di√°logo, vuelve a una ...\n",
      "34    ¬´algunos, dice proclo en su comentario sobre e...\n",
      "35    la composici√≥n perfectamente estudiada del ,al...\n",
      "36    ¬øqu√© quiere decir sofista? la respuesta a esta...\n",
      "37    seg√∫n una tradici√≥n, que no tenemos necesidad ...\n",
      "38    plat√≥n parece presentar a teetetes, cuyo nombr...\n",
      "39    desde las primeras p√°ginas aparece sentado en ...\n",
      "40    el objeto de este di√°logo es la amistad, t√≠tul...\n",
      "41    s√≥crates, en una de las calles de atenas, se e...\n",
      "42    h√© aqu√≠, entre dos di√°logos de algunas l√≠neas,...\n",
      "43    la cuesti√≥n de la naturaleza de lo bello es un...\n",
      "44    el nombre de prot√°goras puesto a la cabeza de ...\n",
      "45    la naturaleza humana o el conocimiento de s√≠ m...\n",
      "46    el verdadero objeto del , no es el valor, sino...\n",
      "47    nada menos complicado que este di√°logo. marcha...\n",
      "48    s√≥crates, que en la , s√≥lo pudo mantenerse fil...\n",
      "49    la apolog√≠a puede dividirse en tres partes, ca...\n",
      "50    la naturaleza de la santidad, o usando el leng...\n",
      "Name: obras, dtype: object\n"
     ]
    }
   ],
   "source": [
    "obras = df_obras[\"obras\"]\n",
    "print(obras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8707ec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Convertir Dataframe a Dataset\n",
    "dataset_obras = Dataset.from_pandas(df_obras[[\"obras\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ee655817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['obras'],\n",
      "    num_rows: 51\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset_obras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "481967fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "['obras']\n",
      "{'obras': Value(dtype='string', id=None)}\n",
      "{'obras': 'la correspondencia que sigue comprende trece cartas, dirigidas una por dion a dionisio y las otras doce por plat√≥n a diferentes personajes contempor√°neos.,las doce cartas de plat√≥n se dividen de esta manera: tres a dionisio, una a dion, dos a los parientes y amigos de dion, una a aristodoro, amigo de dion, dos a arquitas, una a p√©rdicas, una a hermias, erasmo y coriseo, y en fin, una a leodamas.,las primeras, que hacen relaci√≥n a las cosas de sicilia y a los viajes de plat√≥n, son las que tienen alg√∫n inter√©s hist√≥rico; en cuanto al inter√©s filos√≥fico y literario a todas les falta igualmente. bajo cualquier punto de vista que se las considere, estas cartas, aun sin exceptuar la s√©tima, no son en modo alguno dignas de plat√≥n.,\\xa0,\\xa0,\\xa0,\\xa0,\\xa0,\\xa0,'}\n"
     ]
    }
   ],
   "source": [
    "print(type(dataset_obras))\n",
    "print(dataset_obras.column_names)\n",
    "print(dataset_obras.features)\n",
    "print(dataset_obras[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7d256d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre del modelo y ruta del checkpoint\n",
    "model_name = \"PlanTL-GOB-ES/gpt2-base-bne\"\n",
    "#checkpoint_path = r\"C:\\Users\\Lucas\\Desktop\\Socrates\\llm_proyect\\data\\checkpoints\\checkpoint-70\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f807e5b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50265, 768)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar el tokenizador y el modelo\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, unk_token=\"[UNK]\")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name) # cambiar luego a checkpoint path\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b2bb5050",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.max_position_embeddings = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7db6b7e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50265, 768)\n",
       "    (wpe): Embedding(512, 768)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "29ba112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividir_en_chunks(texto, tokenizer, max_length=512):\n",
    "    # Tokenizar el texto sin truncarlo\n",
    "    tokens = tokenizer.encode(texto, truncation=False)\n",
    "    \n",
    "    # Dividir en chunks de tama√±o max_length\n",
    "    chunks = [tokens[i:i+max_length] for i in range(0, len(tokens), max_length)]\n",
    "    \n",
    "    # Convertir los tokens de vuelta a texto\n",
    "    # Convertimos de nuevo los tokens a texto para que el tokenizer los procese correctamente\n",
    "    chunk_texts = [tokenizer.decode(chunk, skip_special_tokens=True) for chunk in chunks]\n",
    "    \n",
    "    return chunk_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "01e94003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['obras'],\n",
      "    num_rows: 57\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def preparar_dataset(dataset_obras, tokenizer, max_length=512):\n",
    "    # Para cada obra, dividirla en chunks\n",
    "    chunks = []\n",
    "    for obra in dataset_obras[\"obras\"]:\n",
    "        chunks.extend(dividir_en_chunks(obra, tokenizer, max_length))\n",
    "    \n",
    "    # Crear un nuevo dataset con los chunks\n",
    "    return Dataset.from_dict({\"obras\": chunks})\n",
    "\n",
    "# Preprocesar dataset: dividir las obras en chunks\n",
    "dataset_obras_dividido = preparar_dataset(df_obras, tokenizer)\n",
    "\n",
    "# Verifica que la estructura del dataset ahora est√© basada en chunks\n",
    "print(dataset_obras_dividido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e90810ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:00<00:00, 215.34 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['obras', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 57\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenization(dataset_obras):\n",
    "    encodings = tokenizer(\n",
    "        dataset_obras[\"obras\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=1024\n",
    "    )\n",
    "\n",
    "    encodings[\"labels\"] = encodings[\"input_ids\"].copy()\n",
    "\n",
    "    # Convertimos a -100 los tokens de padding para que no contribuyan a la p√©rdida\n",
    "    encodings[\"labels\"] = [\n",
    "        [-100 if token == tokenizer.pad_token_id else token for token in labels]\n",
    "        for labels in encodings[\"labels\"]\n",
    "    ]\n",
    "\n",
    "    return {\"input_ids\": encodings[\"input_ids\"], \n",
    "            \"attention_mask\": encodings[\"attention_mask\"], \n",
    "            \"labels\": encodings[\"labels\"]}\n",
    "\n",
    "# Aplicar tokenizaci√≥n al dataset dividido en chunks\n",
    "tokenized_obras = dataset_obras_dividido.map(tokenization, batched=True)\n",
    "\n",
    "# Aseg√∫rate de revisar el nuevo dataset tokenizado\n",
    "print(tokenized_obras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0b399a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['obras', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 57\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_obras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f40c27af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "['obras', 'input_ids', 'attention_mask', 'labels']\n",
      "{'obras': Value(dtype='string', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}\n",
      "{'obras': 'la correspondencia que sigue comprende trece cartas, dirigidas una por dion a dionisio y las otras doce por plat√≥n a diferentes personajes contempor√°neos.,las doce cartas de plat√≥n se dividen de esta manera: tres a dionisio, una a dion, dos a los parientes y amigos de dion, una a aristodoro, amigo de dion, dos a arquitas, una a p√©rdicas, una a hermias, erasmo y coriseo, y en fin, una a leodamas.,las primeras, que hacen relaci√≥n a las cosas de sicilia y a los viajes de plat√≥n, son las que tienen alg√∫n inter√©s hist√≥rico; en cuanto al inter√©s filos√≥fico y literario a todas les falta igualmente. bajo cualquier punto de vista que se las considere, estas cartas, aun sin exceptuar la s√©tima, no son en modo alguno dignas de plat√≥n.,\\xa0,\\xa0,\\xa0,\\xa0,\\xa0,\\xa0,', 'input_ids': [2, 407, 25980, 341, 2375, 16494, 14560, 9050, 66, 12633, 405, 383, 311, 564, 320, 311, 564, 37539, 342, 387, 1312, 7675, 383, 901, 1559, 320, 1817, 4916, 29189, 3790, 610, 7675, 9050, 313, 901, 1559, 361, 38438, 313, 514, 1565, 80, 1113, 320, 311, 564, 37539, 66, 405, 320, 311, 564, 66, 724, 320, 364, 34659, 342, 3128, 313, 311, 564, 66, 405, 320, 32422, 122, 2863, 66, 4545, 313, 311, 564, 66, 724, 320, 582, 29748, 66, 405, 320, 4475, 10440, 66, 405, 320, 44318, 661, 66, 33848, 490, 342, 692, 357, 777, 66, 342, 334, 655, 66, 405, 320, 532, 474, 7835, 3790, 610, 4133, 66, 341, 2762, 2224, 320, 387, 1692, 313, 326, 30479, 342, 320, 364, 5536, 313, 901, 1559, 66, 644, 387, 341, 1386, 2682, 2803, 4979, 81, 334, 2576, 389, 2803, 37309, 342, 15835, 320, 1224, 1138, 1877, 8946, 68, 1912, 1136, 1789, 313, 3006, 341, 361, 387, 17670, 66, 1415, 9050, 66, 6077, 580, 5461, 15313, 332, 3443, 5022, 66, 403, 644, 334, 2546, 5157, 26564, 313, 901, 1559, 3790, 181, 309, 66, 181, 309, 66, 181, 309, 66, 181, 309, 66, 181, 309, 66, 181, 309, 66, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264, 50264], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [2, 407, 25980, 341, 2375, 16494, 14560, 9050, 66, 12633, 405, 383, 311, 564, 320, 311, 564, 37539, 342, 387, 1312, 7675, 383, 901, 1559, 320, 1817, 4916, 29189, 3790, 610, 7675, 9050, 313, 901, 1559, 361, 38438, 313, 514, 1565, 80, 1113, 320, 311, 564, 37539, 66, 405, 320, 311, 564, 66, 724, 320, 364, 34659, 342, 3128, 313, 311, 564, 66, 405, 320, 32422, 122, 2863, 66, 4545, 313, 311, 564, 66, 724, 320, 582, 29748, 66, 405, 320, 4475, 10440, 66, 405, 320, 44318, 661, 66, 33848, 490, 342, 692, 357, 777, 66, 342, 334, 655, 66, 405, 320, 532, 474, 7835, 3790, 610, 4133, 66, 341, 2762, 2224, 320, 387, 1692, 313, 326, 30479, 342, 320, 364, 5536, 313, 901, 1559, 66, 644, 387, 341, 1386, 2682, 2803, 4979, 81, 334, 2576, 389, 2803, 37309, 342, 15835, 320, 1224, 1138, 1877, 8946, 68, 1912, 1136, 1789, 313, 3006, 341, 361, 387, 17670, 66, 1415, 9050, 66, 6077, 580, 5461, 15313, 332, 3443, 5022, 66, 403, 644, 334, 2546, 5157, 26564, 313, 901, 1559, 3790, 181, 309, 66, 181, 309, 66, 181, 309, 66, 181, 309, 66, 181, 309, 66, 181, 309, 66, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}\n"
     ]
    }
   ],
   "source": [
    "print(type(tokenized_obras))\n",
    "print(tokenized_obras.column_names)\n",
    "print(tokenized_obras.features)\n",
    "print(tokenized_obras[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c3cd3726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir a formato PyTorch\n",
    "tokenized_obras.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Configurar token de padding\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eac65636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "['obras', 'input_ids', 'attention_mask', 'labels']\n",
      "{'obras': Value(dtype='string', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}\n",
      "{'input_ids': tensor([    2,   407, 25980,  ..., 50264, 50264, 50264]), 'attention_mask': tensor([1, 1, 1,  ..., 0, 0, 0]), 'labels': tensor([    2,   407, 25980,  ...,  -100,  -100,  -100])}\n"
     ]
    }
   ],
   "source": [
    "print(type(tokenized_obras))\n",
    "print(tokenized_obras.column_names)\n",
    "print(tokenized_obras.features)\n",
    "print(tokenized_obras[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b87d7e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Definir rutas\n",
    "modelo_final_path = r\"C:\\Users\\Lucas\\Desktop\\Socrates\\llm_proyect\\src\\modelo_obras\"\n",
    "checkpoints_path = r\"C:\\Users\\Lucas\\Desktop\\Socrates\\llm_proyect\\data\\checkpoints\"\n",
    "\n",
    "# Asegurar que las carpetas existen\n",
    "os.makedirs(modelo_final_path, exist_ok=True)\n",
    "os.makedirs(checkpoints_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5fd0bbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 18\u001b[0m\n\u001b[0;32m      2\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m      3\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39mcheckpoints_path,\n\u001b[0;32m      4\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m,          \u001b[38;5;66;03m# Tasa de aprendizaje ajustada\u001b[39;00m\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Inicializar Trainer con dataset tokenizado\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_obras\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Usamos el dataset tokenizado\u001b[39;49;00m\n\u001b[0;32m     22\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\deprecation.py:165\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS):\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:453\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_func \u001b[38;5;241m=\u001b[39m compute_loss_func\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# Seed must be set before instantiating the model when using model\u001b[39;00m\n\u001b[1;32m--> 453\u001b[0m enable_full_determinism(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mseed) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mfull_determinism \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mset_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhp_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer_utils.py:105\u001b[0m, in \u001b[0;36mset_seed\u001b[1;34m(seed, deterministic)\u001b[0m\n\u001b[0;32m    103\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[1;32m--> 105\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmanual_seed_all(seed)\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;66;03m# ^^ safe to call this function even if cuda is not available\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_compile.py:32\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     29\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[0;32m     30\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    630\u001b[0m prior \u001b[38;5;241m=\u001b[39m _maybe_set_eval_frame(callback)\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    634\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\random.py:46\u001b[0m, in \u001b[0;36mmanual_seed\u001b[1;34m(seed)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuda\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_is_in_bad_fork():\n\u001b[1;32m---> 46\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmps\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39m_is_in_bad_fork():\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\cuda\\random.py:129\u001b[0m, in \u001b[0;36mmanual_seed_all\u001b[1;34m(seed)\u001b[0m\n\u001b[0;32m    126\u001b[0m         default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[i]\n\u001b[0;32m    127\u001b[0m         default_generator\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[1;32m--> 129\u001b[0m \u001b[43m_lazy_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_all\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\cuda\\__init__.py:249\u001b[0m, in \u001b[0;36m_lazy_call\u001b[1;34m(callable, **kwargs)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_lazy_call\u001b[39m(\u001b[38;5;28mcallable\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[1;32m--> 249\u001b[0m         \u001b[38;5;28;43mcallable\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    251\u001b[0m         \u001b[38;5;66;03m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[39;00m\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;66;03m# file system to get traceback info. Patch linecache or do something\u001b[39;00m\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;66;03m# else here if this ends up being important.\u001b[39;00m\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28;01mglobal\u001b[39;00m _lazy_seed_tracker\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\cuda\\random.py:127\u001b[0m, in \u001b[0;36mmanual_seed_all.<locals>.cb\u001b[1;34m()\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(device_count()):\n\u001b[0;32m    126\u001b[0m     default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[i]\n\u001b[1;32m--> 127\u001b[0m     default_generator\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Configuraci√≥n de entrenamiento obras\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=checkpoints_path,\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=20,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    gradient_accumulation_steps=2, # Acumular gradientes cada 2 pasos\n",
    "    learning_rate=1e-5,          # Tasa de aprendizaje ajustada\n",
    ")\n",
    "\n",
    "# Inicializar Trainer con dataset tokenizado\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_obras  # Usamos el dataset tokenizado\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ff29510",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Entrenar modelo obras\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2531\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2524\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2525\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2528\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2529\u001b[0m )\n\u001b[0;32m   2530\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2531\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2534\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2535\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2536\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2537\u001b[0m ):\n\u001b[0;32m   2538\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2539\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:3675\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3674\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3675\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3677\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3679\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3680\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3681\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:3731\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3729\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[0;32m   3730\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[1;32m-> 3731\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3732\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3733\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1061\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1059\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1061\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1062\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1073\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1074\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1075\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1076\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:829\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    827\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m attention_mask \u001b[38;5;28;01mif\u001b[39;00m (attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01min\u001b[39;00m attention_mask) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _use_sdpa:\n\u001b[1;32m--> 829\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_4d_causal_attention_mask_for_sdpa\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    836\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    837\u001b[0m         \u001b[38;5;66;03m# We create a 3D attention mask from a 2D tensor mask.\u001b[39;00m\n\u001b[0;32m    838\u001b[0m         \u001b[38;5;66;03m# Sizes are [batch_size, 1, 1, to_seq_length]\u001b[39;00m\n\u001b[0;32m    839\u001b[0m         \u001b[38;5;66;03m# So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\u001b[39;00m\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;66;03m# this attention mask is more simple than the triangular masking of causal attention\u001b[39;00m\n\u001b[0;32m    841\u001b[0m         \u001b[38;5;66;03m# used in OpenAI GPT, we just need to prepare the broadcast dimension here.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:378\u001b[0m, in \u001b[0;36m_prepare_4d_causal_attention_mask_for_sdpa\u001b[1;34m(attention_mask, input_shape, inputs_embeds, past_key_values_length, sliding_window)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;66;03m# torch.jit.trace, symbolic_trace and torchdynamo with fullgraph=True are unable to capture the controlflow `is_causal=attention_mask is None and q_len > 1`\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# used as an SDPA argument. We keep compatibility with these tracing tools by always using SDPA's `attn_mask` argument in case we are tracing.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;66;03m# TODO: For dynamo, rather use a check on fullgraph=True once this is possible (https://github.com/pytorch/pytorch/pull/120400).\u001b[39;00m\n\u001b[0;32m    376\u001b[0m is_tracing \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs_embeds, torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mProxy) \u001b[38;5;129;01mor\u001b[39;00m is_torchdynamo_compiling()\n\u001b[1;32m--> 378\u001b[0m ignore_causal_mask \u001b[38;5;241m=\u001b[39m \u001b[43mAttentionMaskConverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ignore_causal_mask_sdpa\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43msliding_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msliding_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_causal_mask:\n\u001b[0;32m    386\u001b[0m     expanded_4d_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:288\u001b[0m, in \u001b[0;36mAttentionMaskConverter._ignore_causal_mask_sdpa\u001b[1;34m(attention_mask, inputs_embeds, past_key_values_length, sliding_window, is_training)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(attention_mask\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 288\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tracing \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mall(attention_mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query_length \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key_value_length \u001b[38;5;241m==\u001b[39m query_length:\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;66;03m# For query_length == 1, causal attention and bi-directional attention are the same.\u001b[39;00m\n\u001b[0;32m    291\u001b[0m         ignore_causal_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Entrenar modelo obras\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9933711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\Lucas\\\\Desktop\\\\Socrates\\\\llm_proyect\\\\src\\\\modelo_obras\\\\tokenizer_config.json',\n",
       " 'C:\\\\Users\\\\Lucas\\\\Desktop\\\\Socrates\\\\llm_proyect\\\\src\\\\modelo_obras\\\\special_tokens_map.json',\n",
       " 'C:\\\\Users\\\\Lucas\\\\Desktop\\\\Socrates\\\\llm_proyect\\\\src\\\\modelo_obras\\\\vocab.json',\n",
       " 'C:\\\\Users\\\\Lucas\\\\Desktop\\\\Socrates\\\\llm_proyect\\\\src\\\\modelo_obras\\\\merges.txt',\n",
       " 'C:\\\\Users\\\\Lucas\\\\Desktop\\\\Socrates\\\\llm_proyect\\\\src\\\\modelo_obras\\\\added_tokens.json',\n",
       " 'C:\\\\Users\\\\Lucas\\\\Desktop\\\\Socrates\\\\llm_proyect\\\\src\\\\modelo_obras\\\\tokenizer.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Guardar el modelo\n",
    "model.save_pretrained(modelo_final_path)\n",
    "\n",
    "# Guardar el tokenizador\n",
    "tokenizer.save_pretrained(modelo_final_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35bc288d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬øQu√© es la justicia? \n"
     ]
    }
   ],
   "source": [
    "input_text = \"¬øQu√© es la justicia?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "output = model.generate(**inputs, max_length=100)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
